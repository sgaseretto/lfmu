# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks/01_synth_dataset.ipynb (unless otherwise specified).

__all__ = ['gen_values', 'gen_added_n_deleted', 'exclude_element', 'build_interactions', 'build_features_from_df',
           'build_datasets']

# Cell
import pandas as pd
import numpy as np
import random
from lightfm import LightFM
from lightfm.data import Dataset

random.seed(42)

# Cell
def gen_values(n_values=10, prefix='u'):
    "Generates a list of values that will be used for generate the dataset"
    l = []
    for i in range(n_values):
        l.append(prefix + str(i))
    return l

# Cell
def gen_added_n_deleted(l_values, max_added=3, max_deleted=3):
    '''
    Generates two lists of values, one list will contain the values that will be deleted from the dataset,
    and the second one will contain the values that will be added to the dataset.
    '''
    deleted = []
    added = []
    for i in l_values:
        r = random.random()
        if len(deleted) < max_deleted and r < 0.8:
            deleted.append(i)
        elif len(added) < max_added and r > 0.2:
            added.append(i)

    return added, deleted

# Cell
def exclude_element(l, values_to_exclude, shuffle=False):
    "Excludes the elements from **values_to_exclude** from **l**"
    new_l = [x for x in l if set(values_to_exclude).issuperset({x}) == False]
    if shuffle: new_l.shuffle()
    return new_l

# Cell
def build_interactions(l1, l2, l1_col_name='user_id', l2_col_name='item_id', sparsity=0.5):
    '''
    Builds interactions between l1 and l2.
    The sparsity determines how sparse this interactions will be.
    '''
    interactions = {l1_col_name:[], l2_col_name:[]}
    for i in l1:
        for j in l2:
            if random.random() < sparsity:
                interactions[l1_col_name].append(i)
                interactions[l2_col_name].append(j)
    return pd.DataFrame(interactions)

# Cell
def build_features_from_df(feature_interactions_df, element_id_column, feature_column, tolist=True):
    '''
    Builds tuples of elements and its features to build the dataset
    '''
    unique_elements = feature_interactions_df[element_id_column].unique()
    tuples = []
    for e in unique_elements:
        filtered_rows = feature_interactions_df[feature_interactions_df[element_id_column] == e]
        feature_list = filtered_rows[feature_column].unique()
        if tolist: feature_list = feature_list.tolist()
        tuples.append((e, feature_list))
    return tuples

# Cell
def build_datasets(n_users=10, n_items=10, max_added=3, max_deleted=3, print_added_n_deleted=False):
    '''
    This function generates two **datasets** to simulate changes through time from one dataset.
    The first generated **dataset** is the state from the data in a time *t* and the second dataset
    simulates the state from the data at a time *t+1* where some users and items where added and deleted,
    and their metadata could be also updated (new metadata that expresses better the characteristics from that item, or just corrections)
    '''
    before = {}
    after = {}
    users = gen_values(n_values=n_users, prefix='u')
    items = gen_values(n_values=n_items, prefix='i')
    all_user_features = gen_values(prefix='uf')
    all_item_features = gen_values(prefix='if')

    users_added, users_deleted = gen_added_n_deleted(users, max_added=max_added, max_deleted=max_deleted)
    if print_added_n_deleted: print(users_added, users_deleted)
    items_added, items_deleted = gen_added_n_deleted(items, max_added=max_added, max_deleted=max_deleted)
    if print_added_n_deleted: print(items_added, items_deleted)
    before['user_id'] = exclude_element(users, users_added)
    before['item_id'] = exclude_element(items, items_added)
    after['user_id'] = exclude_element(users, users_deleted)
    after['item_id'] = exclude_element(items, items_deleted)
    return before, after